{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bdf7099-a6d0-4b18-9216-f8e6701842f6",
   "metadata": {},
   "source": [
    "# Laboratory 4: Getting started with Pytorch\n",
    "\n",
    "In this laboratory we will begin working with Pytorch to implement and train complex, nonlinear models for supervised learning problems. You will notice many similarities between Numpy and Pytorch -- this is deliberate, but it can cause some confusion and for many things we will have to convert back and forth between Numpy arrays and Pytorch tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3117a44-c264-469e-91b6-2cc445413903",
   "metadata": {},
   "source": [
    "## Part 0: First steps\n",
    "\n",
    "**Important**: You **must** install Pytorch in your Anaconda environment for this laboratory. The easiest way to do this is to just install the CPU version of Pytorch like this:\n",
    "\n",
    "```\n",
    "conda activate FML\n",
    "conda install -c pytorch pytorch torchvision\n",
    "```\n",
    "\n",
    "**Note**: If you have an Nvidia GPU on your computer you can also install the GPU-enabled version of Pytorch which will **greatly** improve performance for more complex models and larger datasets. However, it can be very hard to get all of the versions of the required libraries to match correctly... During the laboratory we can look at it together if you are interested.\n",
    "\n",
    "After installing Pytorch, use the next cell to verify that the installation is working. If it prints a 3x3 sensor, we're good to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcdca90-bf99-4c35-b7e3-961f6ba45cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're still going to need numpy and matplotlib.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Verify that pytorch is working.\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "\n",
    "\n",
    "print(torch.backends.mps.is_available()) #the MacOS is higher than 12.3+\n",
    "print(torch.backends.mps.is_built()) #MPS is activated\n",
    "\n",
    "foo = torch.randn((3, 3))\n",
    "print(foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6f7466-0f8f-406e-a4e7-24e8a8135f8a",
   "metadata": {},
   "source": [
    "## Part 1: Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebc2242-e79a-4e67-af09-8318126dde0e",
   "metadata": {},
   "source": [
    "We will work with the venerable MNIST dataset of handwritten digits in this laboratory. The `torchvision` library provides classes for a bunch of standard datasets, including MNIST. These classes automatically download and prepare the dataset for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af0151f-9897-4fa0-8832-4603979427df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the MNIST dataset.\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision\n",
    "\n",
    "# Load the MNIST training and test splits.\n",
    "ds_train = MNIST(root='./data', download=True, train=True)\n",
    "ds_test  = MNIST(root='./data', download=True, train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f7fc9b-2a69-48eb-bea1-9cf97117ebf8",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Exploratory data analysis\n",
    "\n",
    "Spend some time inspecting the `ds_train` and `ds_test` data structures in order to get a feel for the data. What is the format? How big are the images? How many are there? What about the range of pixel values? Where are the labels for images?\n",
    "\n",
    "Remember that one of the best ways to explore is to *visualize*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81157a27-3c60-432d-b3a0-6b6741f42747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f462b0-0bd1-4501-8b96-d34e8e873e8a",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Dataset conversion and normalization\n",
    "\n",
    "+ **Datatype Conversion**:\n",
    "The first thing we need to do is convert all data tensors to `torch.float32` -- this is fundamental as it is extremely inconvenient to work with `uint8` data. Using 32-bit floating point numbers is a compromise between precision and space efficiency.\n",
    "The `torch.Tensor` class has a very useful method `to()` for performing datatype and device (e.g. to GPU) conversions. Check out the [documentation here](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch-tensor-to).\n",
    "\n",
    "+ **Normalization**:\n",
    "Next, we need to correct the inconvenient range of [0, 255] for the pixel values. You should *subtract* the mean intensity value and divide by the standard deviation in order to *standardize* our data. **Important**: Think *very carefully* about *which* split you should use to compute the pixel statistics for standardization.\n",
    "\n",
    "+ **Reshaping**: Is the data in an appropriate format (i.e. shape) for the training the models we know? Think about whether (and how) to fix this if needed. \n",
    "\n",
    "**What to do**: In the cell below you should perform this sequence preprocessing operations on the `ds_train.data` and `ds_test.data` tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b71841-b3a6-4887-8c62-450aa4b36b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68fcf34-8efe-4224-8138-8739d6c2c23f",
   "metadata": {},
   "source": [
    "### Exercise 1.3: Subsampling the MNIST dataset.\n",
    "\n",
    "MNIST is kind of big, and thus inconvenient to work with unless using the GPU. For this laboratory we will use a smaller subset of the dataset for training to keep memory and computation times low.\n",
    "\n",
    "Modify `ds.train` to use only a subset of, say, 10000 images sampled from the original data. Make sure to select the correct corresponding targets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d2ea58-1f86-42fe-b867-f824e4966ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d557069-7f6d-4765-938f-6ff3e360b15d",
   "metadata": {},
   "source": [
    "## Establishing a stable baseline\n",
    "\n",
    "In this exercise you will establish a reliable baseline using a classical approach. This is an important step in our methodology in order to judge whether our Deep MLP is performing well or not.\n",
    "\n",
    "### Exercise 2.1: Establish the stable baseline\n",
    "\n",
    "Train and test your stable baseline to estimate the best achievable accuracy using classical models.\n",
    "\n",
    "**Tip**: Don't do any extensive cross-validation of your baseline (for now). Just fit a simple model (e.g. a linear SVM) and record the accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf8cf7-0632-46fa-b97e-32eef1f166ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215ded91-0fc9-4923-883f-548adebbc4a1",
   "metadata": {},
   "source": [
    "## Part 3: Training some deep models (finally)\n",
    "\n",
    "Now we will finally train some deep models (Multilayer Perceptrons, to be precise). Since the dataset is a bit too large to use batch gradient descent, we will first need to setup a `torch.utils.data.DataLoader` for our training data. A `DataLoader` breaks the dataset up into a sequence of *batches* that will be used for training. In order to use this, we will first have to use `torch.utils.data.TensorDataset` on `ds_train.data` and `ds_train.targets` to make a new torch `dataset` for use in the dataloader. \n",
    "\n",
    "### Exercise 3.1: Creating the DataLoader\n",
    "\n",
    "Create a `DataLoader` for `ds_train` use a `batch_size` of about 16 or 32 to start. After you have your `DataLoader` experiment with is using `next(iter(dl_train))` to see what it returns. The pytorch `DataLoader` is a Python iterator.\n",
    "\n",
    "**EXTREMELY IMPORTANT**: Make sure you use `shuffle=True` in the constructor of your dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf280cd7-418f-4ed4-9ae8-4bb9a618e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here.\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c40712-538f-4efa-9752-f58a866d357f",
   "metadata": {},
   "source": [
    "### Some support code (NOT an exercise).\n",
    "\n",
    "Here is some support code that you can use to train a model for a **single** epoch. The function returns the mean loss over all iterations. You will use it in the next exercise to train and monitor training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638ed76b-b385-46d2-904e-f2ee31b9a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model for a single epoch. You should pass it a model, a dataloader,\n",
    "# and an optimizer. Returns the mean loss over the entire epoch.\n",
    "def train_epoch(model, dl, optimizer):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for (xs, ys) in dl:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(xs)\n",
    "        loss = torch.nn.functional.nll_loss(output, ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    model.eval()\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaaaf4a-7266-43b8-a5b6-c36c7cb087bc",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Defining a 1-layer neural network\n",
    "\n",
    "Define a simple model that uses a **single** `torch.nn.Linear` layer followed by a `torch.nn.Softmax` to predict  the output probabilities for the ten classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f6ae8-5826-44ab-9077-6358f7585f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fresh model.\n",
    "model = torch.nn.Sequential(\n",
    "    # Your code here.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0892210c-7b7c-4bab-a648-121d52190cb1",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Training our model\n",
    "\n",
    "Instantiate a `torch.optim.SGD` optimizer using `model.parameters()` and the learning rate (**tip**: make the learning rate variable you can easily change). Then run `train_epoch` for a set number of epochs (e.g. 100, make this a variable too). Is your model learning? How can you tell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca7a8f-f7a9-4536-a4ba-51ab0e9fbead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2209ce-48d6-4412-9973-456c2d0ae1f0",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Evaluating our model\n",
    "\n",
    "Write some code to plot the loss curve for your training run and evaluate the performance of your model on the test data. Play with the hyperparameters (e.g. learning rate) to try to get the best performance on the test set. Can you beat the stable baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e688ba-cc3f-4445-ab6b-6675a446ba1c",
   "metadata": {},
   "source": [
    "## Going Deeper\n",
    "\n",
    "Now we will go (at least one layer) deeper to see if we can significantly improve on the baseline.\n",
    "\n",
    "### Exercise 3.4: A 2-layer MLP\n",
    "Define a new model with one hidden layer. Use the code you wrote above to train and evaluate this new model. Can you beat the baseline? You might need to train in two stages using different learning rates.\n",
    "\n",
    "**Things to think about**:\n",
    "\n",
    "+ It might be hard to beat (or even equal) the baseline with deeper networks. Why?\n",
    "+ Is there something else we should be monitoring while training, especially for deep networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c9686-49d7-4bc3-bbd9-0671547bd610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
