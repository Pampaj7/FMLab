{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a54f1d43",
   "metadata": {},
   "source": [
    "# Laboratory 5: Convolutional Neural Networks\n",
    "\n",
    "In this laboratory session we will train some CNNs to recognize color images in the [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbcd5fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1: Initial Setup and Data Exploration\n",
    "\n",
    "We begin with some standard imports, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0017bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "# Standard Pytorch imports (note the aliases).\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607cc4cb",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Dataset and Dataloader Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5488263d",
   "metadata": {},
   "source": [
    "The `torchvision` library provides a class (with the same interface as MNIST) for the CIFAR-10 dataset. As with MNIST, it will automatically download and prepare the dataset for use. Use the CIFAR10 class to load the training, validation (use 2000 images), and test splits.\n",
    "\n",
    "**Note**: Don't forget to *transform* the images in the datasets to convert them to tensors and standardize them!\n",
    "\n",
    "**Hint**: Feel free to copy-and-paste liberally from the notebook I published for the capsule lecture. **BUT**, make sure you know what you are doing, and be aware that *some* of the code will have to be adapted for use with the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e468226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Validation set size.\n",
    "val_size = 2000\n",
    "\n",
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be2f0e",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Dataloaders\n",
    "Set up dataloaders for **all** of the datasets -- even though the validation set is small! Test out the datasets defined above and the dataloaders to make sure you understand the dataset format. Visualize some of the images to get a feel for the type of images and classes in CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e84efdf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setup dataloaders for all three datasets. Use the largest batch size possible.\n",
    "batch_size = 256\n",
    "\n",
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a759bbd",
   "metadata": {},
   "source": [
    "## Part 2: Establishing a stable baseline\n",
    "\n",
    "In this part of the laboratory we will establish a simple baseline as a starting point.\n",
    "\n",
    "### Exercise 2.1: An MLP Baseline\n",
    "\n",
    "Define a simple Multilayer Perceptron to classify the CIFAR-10 images. Define it as a class inheriting from torch.nn.Module. Don't make it too complex or too deep. We're just looking for a starting point. A *baseline*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02b67295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98427e54",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Train and Evaluate your MLP Baseline\n",
    "\n",
    "Train the model for a few (say, 20) epochs. Again, feel free to use my training code from the Capsule Lecture (or roll your own, mine is very basic). Make sure you plot training curves and report accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cc96ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14689935",
   "metadata": {},
   "source": [
    "## Part 3: A CNN for CIFAR-10 Classification\n",
    "\n",
    "OK, we have a (simple) MLP baseline for comparison. Let's implement a simple CNN to classify CIFAR-10 images and see if we can beat the MLP.\n",
    "\n",
    "### Exercise 3.1: Defining the CNN\n",
    "\n",
    "Define a simple CNN model with a few convolutional and maxppooling layers -- not too many, since CIFAR-10 images are only 32x32 pixels! Use two fully-connected layers after the last convolution and before the logit outputs. Test out the model by passing a *single* image through it to make sure it's working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5247ae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e4c413-6428-49db-a877-32f7ee2c67c1",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Training and Evaluating your CNN\n",
    "\n",
    "Train the CNN using similar hyperparameters to what you used for the MLP above (epochs, learning rate). Evaluate the model in the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a98f9664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75dd514-4970-473c-84c5-4632b32fd599",
   "metadata": {},
   "source": [
    "## Going Forward\n",
    "\n",
    "In practice we usually don't train deep models from *scratch*. Especially if we don't have a lot of annotated data we almost always use a **pre-trained** model either as a **feature extractor** or to **fine-tune** on our problem. The Torchvision library supports access to a [huge variety or pre-trained models](https://pytorch.org/vision/stable/models/resnet.html) that you can use for *exactly* this purpose. Keep this in mind if you have an image recognition problem -- you can use a pre-trained model as a **feature extractor** and then train a *simple* MLP to solve your classification problem. This works *very* well in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04809f3-3577-43a4-8f88-fca759137d68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
